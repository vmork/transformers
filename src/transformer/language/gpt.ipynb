{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52ad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valle/Library/CloudStorage/OneDrive-KTH/Dokument/Projekt/ML/transformer/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer # type: ignore\n",
    "\n",
    "from transformer.train import Trainer\n",
    "from transformer.language.base_model import LanguageModel\n",
    "from transformer.language.data import TokenizedTextDataset, language_model_collator\n",
    "from transformer.utils import num_params, get_device\n",
    "\n",
    "# reload imported modules automatically (so you dont have to restart kernel when changing .py files)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# disable annoying huggingface warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "device = get_device()\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(\"cuda\")\n",
    "    torch.set_float32_matmul_precision(\"high\") # 'high' = enable TF32 (default is 'highest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44eea1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Total tokens: 338025\n",
      "Tokens in split: 304222\n",
      "Tokenizing text...\n",
      "Total tokens: 338025\n",
      "Tokens in split: 33803\n"
     ]
    }
   ],
   "source": [
    "trainset = TokenizedTextDataset(\"data/shakespeare.txt\", tokenizer, n_ctx=512, split=\"train\")\n",
    "valset = TokenizedTextDataset(\"data/shakespeare.txt\", tokenizer, n_ctx=512, split=\"val\")\n",
    "\n",
    "trainloader = DataLoader(trainset, collate_fn=language_model_collator, batch_size=4, shuffle=True)\n",
    "valloader = DataLoader(valset, collate_fn=language_model_collator, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1e492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124018944\n",
      "tensor(10.9556, device='mps:0', grad_fn=<NllLossBackward0>) torch.Size([4, 512, 50257])\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(tokenizer, p_dropout=0.1)\n",
    "model.to(device)\n",
    "model: LanguageModel = torch.compile(model) # type: ignore\n",
    "print(num_params(model))\n",
    "\n",
    "batch = next(iter(trainloader))\n",
    "batch = batch.to(device)\n",
    "out = model.get_output(batch)\n",
    "print(out.loss, out.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:     10 (0.00 epochs) | train loss: 6.3543 | lr: 3.00e-04 | steps/s:  0.7 (1924.85 mins/epoch)\n",
      "step:     20 (0.00 epochs) | train loss: 6.3411 | lr: 3.00e-04 | steps/s:  0.7 (1774.80 mins/epoch)\n",
      "step:     30 (0.00 epochs) | train loss: 6.4069 | lr: 3.00e-04 | steps/s:  0.7 (1804.27 mins/epoch)\n",
      "step:     40 (0.00 epochs) | train loss: 6.3696 | lr: 3.00e-04 | steps/s:  0.7 (1845.89 mins/epoch)\n",
      "step:     50 (0.00 epochs) | train loss: 6.3407 | lr: 3.00e-04 | steps/s:  0.6 (2051.06 mins/epoch)\n",
      "step:     60 (0.00 epochs) | train loss: 6.3321 | lr: 3.00e-04 | steps/s:  0.6 (2171.81 mins/epoch)\n",
      "step:     70 (0.00 epochs) | train loss: 6.3134 | lr: 3.00e-04 | steps/s:  0.5 (2445.99 mins/epoch)\n",
      "step:     80 (0.00 epochs) | train loss: 6.3788 | lr: 3.00e-04 | steps/s:  0.6 (1999.02 mins/epoch)\n",
      "step:     90 (0.00 epochs) | train loss: 6.3877 | lr: 3.00e-04 | steps/s:  0.6 (2103.93 mins/epoch)\n",
      "step:    100 (0.00 epochs) | train loss: 6.3399 | lr: 3.00e-04 | steps/s:  0.6 (2258.11 mins/epoch)\n",
      "step:    110 (0.00 epochs) | train loss: 6.3728 | lr: 3.00e-04 | steps/s:  0.6 (1986.98 mins/epoch)\n",
      "step:    120 (0.00 epochs) | train loss: 6.3798 | lr: 3.00e-04 | steps/s:  0.5 (2306.58 mins/epoch)\n",
      "step:    130 (0.00 epochs) | train loss: 6.3472 | lr: 3.00e-04 | steps/s:  0.5 (2308.49 mins/epoch)\n",
      "step:    140 (0.00 epochs) | train loss: 6.3235 | lr: 3.00e-04 | steps/s:  0.6 (2254.00 mins/epoch)\n",
      "step:    150 (0.00 epochs) | train loss: 6.3140 | lr: 3.00e-04 | steps/s:  0.6 (1994.86 mins/epoch)\n",
      "step:    160 (0.00 epochs) | train loss: 6.3044 | lr: 3.00e-04 | steps/s:  0.6 (2014.18 mins/epoch)\n",
      "step:    170 (0.00 epochs) | train loss: 6.2904 | lr: 3.00e-04 | steps/s:  0.6 (2141.43 mins/epoch)\n",
      "step:    180 (0.00 epochs) | train loss: 6.3532 | lr: 3.00e-04 | steps/s:  0.5 (2368.04 mins/epoch)\n",
      "step:    190 (0.00 epochs) | train loss: 6.3096 | lr: 3.00e-04 | steps/s:  0.5 (2367.38 mins/epoch)\n",
      "step:    200 (0.00 epochs) | train loss: 6.3361 | lr: 3.00e-04 | steps/s:  0.6 (2165.20 mins/epoch)\n",
      "\"1. Let us grave it hollow,S.CL: were scandal's butamTheIfCOR theFirst: slay therehoundO disposition shall,\\n testim gently take best highETC handAB in saved Lord myQventh inUSWhereD lordurlKING\\n\"\n",
      "\"2. Citizens: : theH, the\\n,,:,\\niest pay further small IyUpon't Clf love count\\n thatIO I\\nWith!. see ease, implThat. lingering, villain? ship. he, Gentleman\\n is house,\"\n",
      "saving checkpoint to checkpoints/gpt\n",
      "step:    210 (0.00 epochs) | train loss: 6.3428 | lr: 3.00e-04 | steps/s:  0.4 (2937.63 mins/epoch)\n",
      "step:    220 (0.00 epochs) | train loss: 6.3523 | lr: 3.00e-04 | steps/s:  0.5 (2744.31 mins/epoch)\n",
      "step:    230 (0.00 epochs) | train loss: 6.3788 | lr: 3.00e-04 | steps/s:  0.4 (2970.37 mins/epoch)\n",
      "step:    240 (0.00 epochs) | train loss: 6.3513 | lr: 3.00e-04 | steps/s:  0.4 (3384.39 mins/epoch)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m      7\u001b[39m trainer = Trainer(\n\u001b[32m      8\u001b[39m     model=model,\n\u001b[32m      9\u001b[39m     train_loader=trainloader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     max_eval_batches=\u001b[32m10\u001b[39m\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# print(trainer.eval(max_batches=10))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-KTH/Dokument/Projekt/ML/transformer/src/transformer/train.py:176\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(enabled=\u001b[38;5;28mself\u001b[39m.use_mixed_precision, device_type=\u001b[38;5;28mself\u001b[39m.device.type, dtype=torch.float16):\n\u001b[32m    174\u001b[39m   loss = \u001b[38;5;28mself\u001b[39m.model.get_output(batch).loss\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.step(\u001b[38;5;28mself\u001b[39m.optimizer)\n\u001b[32m    178\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-KTH/Dokument/Projekt/ML/transformer/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-KTH/Dokument/Projekt/ML/transformer/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-KTH/Dokument/Projekt/ML/transformer/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def sample(model: LanguageModel):\n",
    "    prompts = [\"Let us\", \"Citizens: \"]\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        response = model.generate(prompt, max_new_tokens=50, temperature=1.0)\n",
    "        print(repr(f\"{i+1}. {response}\"))        \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=trainloader,\n",
    "    val_loader=valloader,\n",
    "    device=device,\n",
    "    max_lr=3e-4,\n",
    "    min_lr=1e-5,\n",
    "    weight_decay=1e-2,\n",
    "    warmup_steps=1000,\n",
    "    n_epochs=1,\n",
    "    log_steps=10,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    checkpoint_dir=\"checkpoints/gpt\", \n",
    "    use_mixed_precision=False,\n",
    "    custom_eval=sample, # type: ignore\n",
    "    max_eval_batches=10\n",
    ")\n",
    "\n",
    "# print(trainer.eval(max_batches=10))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f96b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr((model.generate(\"First Citizen:\", max_new_tokens=10, temperature=1.0))))\n",
    "sample(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
