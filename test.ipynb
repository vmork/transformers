{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65b51ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16628630",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = 30522\n",
    "n_ctx = 512\n",
    "n_layers = 12\n",
    "n_head = 12\n",
    "\n",
    "d_emb = 768\n",
    "d_head = 64\n",
    "d_mlp = 3072\n",
    "\n",
    "assert d_head*n_head == d_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_emb, d_mlp):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_emb, d_mlp)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.w2 = nn.Linear(d_mlp, d_emb)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [B, N, d_emb] -> [B, N, d_emb]\n",
    "        return self.w2(self.relu(self.w1(x)))\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_emb, d_head):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "        self.wq = nn.Linear(d_emb, d_head, bias=False)\n",
    "        self.wk = nn.Linear(d_emb, d_head, bias=False)\n",
    "        self.wv = nn.Linear(d_emb, d_head, bias=False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [B, T, d_emb] -> [B, T, d_head]\n",
    "        q = self.wq(x) # [B, T, d_head]\n",
    "        k = self.wk(x) # [B, T, d_head]\n",
    "        v = self.wv(x) # [B, T, d_head]\n",
    "        A = (q @ k.transpose(-1, -2)) / self.d_head**0.5 # [B, T, T]\n",
    "        A = F.softmax(A, dim=-1) # softmax along key dimension\n",
    "        x = A @ v\n",
    "        return x\n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_emb, d_head):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(d_emb=d_emb, d_head=d_head) for _ in range(n_head)])\n",
    "        self.wo = nn.Linear(d_emb, d_emb) # == torch.cat([wo_1, ..., wo_12])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [B, T, d_emb] -> [B, T, d_emb]\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1) # [B, T, d_head*n_head] = [B, T, d_emb]\n",
    "        x = self.wo(x) # equivalent to `sum(wo_i * h_i(x) for i in range(n_layers))`\n",
    "        return x\n",
    "\n",
    "class EfficientMultiHeadAttention(nn.Module):\n",
    "    # Do everything at the same time!\n",
    "    def __init__(self, n_head, d_emb, d_head, masked_attention=False):\n",
    "        super().__init__()\n",
    "        self.n_head, self.d_head, self.masked_attention = n_head, d_head, masked_attention\n",
    "        # q,k,v matrices for all heads, stacked along last dim\n",
    "        self.wqkv = nn.Linear(d_emb, 3*d_emb, bias=False)\n",
    "        if masked_attention:\n",
    "            self.register_buffer('mask', torch.tril(torch.ones((1, 1, n_ctx, n_ctx))))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # [B, T, d_emb] -> [B, T, d_emb]\n",
    "        B, T, d_emb = x.shape\n",
    "        assert d_emb == self.n_head*self.d_head\n",
    "        \n",
    "        # q,k,v: [B, T, d_emb = n_head*d_head]\n",
    "        q, k, v = self.wqkv(x).split(d_emb, dim=-1) \n",
    "        # reshape to q,k,v: [B, n_head, T, d_head], so that we can compute all attention heads as batched mm\n",
    "        q = q.view(B, T, self.n_head, self.d_head).transpose(1, 2) # [B, n_head, T, d_head]\n",
    "        k = k.view(B, T, self.n_head, self.d_head).transpose(1, 2) # [B, n_head, T, d_head]\n",
    "        v = v.view(B, T, self.n_head, self.d_head).transpose(1, 2) # [B, n_head, T, d_head]\n",
    "        \n",
    "        A = (q @ k.transpose(-1, -2)) / self.d_head**0.5 # [B, n_head, T, T]\n",
    "        if self.masked_attention:\n",
    "            A = torch.masked_fill(A, self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        A = F.softmax(A, dim=-1)\n",
    "        \n",
    "        x = A @ v # [B, n_head, T, d_head]\n",
    "        x = x.transpose(1, 2).reshape(B, T, d_emb)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_head, d_emb, d_mlp, d_head, masked_attention=False):\n",
    "        super().__init__()\n",
    "        self.attn = EfficientMultiHeadAttention(n_head=n_head, d_emb=d_emb, d_head=d_head, masked_attention=masked_attention)\n",
    "        self.mlp = MLP(d_emb=d_emb, d_mlp=d_mlp)\n",
    "        self.ln1 = nn.LayerNorm(d_emb)\n",
    "        self.ln2 = nn.LayerNorm(d_emb)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [B, T, d_emb] -> [B, T, d_emb]\n",
    "        x = self.ln1(self.attn(x) + x)\n",
    "        x = self.ln2(self.mlp(x) + x)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_layers=12, n_head=12, n_ctx=512, d_emb=768, d_head=64, d_mlp=3072, p_dropout=0.1, masked_attention=False):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(n_vocab, d_emb)\n",
    "        self.pos_emb = nn.Embedding(n_ctx, d_emb)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(n_head=n_head, d_emb=d_emb, d_mlp=d_mlp, d_head=d_head, masked_attention=masked_attention)\n",
    "                                     for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, T] (ints of token ids) -> [B, T, d_emb]\n",
    "        B, T = x.shape\n",
    "        xe = self.tok_emb(x) # [B, T, d_emb]\n",
    "        xp = self.pos_emb(torch.arange(0, T)) # [T, d_emb]\n",
    "        x = xe + xp # [B, T, d_emb]\n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ce1e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "bert = Transformer(masked_attention=False)\n",
    "gpt = Transformer(masked_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4be209b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 7592, 1010, 2026,  103, 2003, 3960,  102]]) torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 768]),\n",
       " tensor([[[ 2.9875,  0.3583, -0.8207,  ..., -0.1635,  0.0303, -1.5485],\n",
       "          [ 3.2522,  0.0058, -0.3906,  ...,  0.2550,  0.0067, -0.7064],\n",
       "          [ 3.0829,  0.0514, -0.8678,  ...,  0.1277, -0.2871, -0.8008],\n",
       "          ...,\n",
       "          [ 2.5182, -0.1373, -0.9167,  ...,  0.0344,  0.2996, -0.9848],\n",
       "          [ 2.5908,  0.1701, -0.3427,  ..., -0.2808,  0.2711, -0.5008],\n",
       "          [ 2.1048,  0.7076, -0.0593,  ..., -1.1217, -0.0201, -0.9383]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hello, my [MASK] is Bob\"\n",
    "tok_ids = tok(text)[\"input_ids\"]\n",
    "x = torch.tensor(tok_ids).unsqueeze(0)\n",
    "print(x, x.shape)\n",
    "\n",
    "y = gpt(x)\n",
    "\n",
    "y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d40a6f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.6273,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "              -inf,    -inf,    -inf],\n",
       "          [-1.6599, -0.8810,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "              -inf,    -inf,    -inf],\n",
       "          [-0.6337,  1.3482, -0.5252,    -inf,    -inf,    -inf,    -inf,\n",
       "              -inf,    -inf,    -inf],\n",
       "          [ 0.3444,  0.6288, -0.3466,  2.1626,    -inf,    -inf,    -inf,\n",
       "              -inf,    -inf,    -inf],\n",
       "          [-1.8394,  1.7015, -0.1831,  1.4169, -0.0463,    -inf,    -inf,\n",
       "              -inf,    -inf,    -inf],\n",
       "          [-0.6611,  1.9132, -0.7888,  1.0590, -0.9418, -1.5726,    -inf,\n",
       "              -inf,    -inf,    -inf],\n",
       "          [-0.3688,  0.5541,  1.5305, -0.7085, -1.5465, -0.4701,  0.1381,\n",
       "              -inf,    -inf,    -inf],\n",
       "          [ 0.1376,  1.5309, -0.0048, -0.7227,  0.7957,  0.3171, -0.9580,\n",
       "            1.0536,    -inf,    -inf],\n",
       "          [-2.2465,  0.7787,  0.0297, -0.1052, -1.7899,  0.7464,  0.5787,\n",
       "           -0.5467,  0.5192,    -inf],\n",
       "          [-2.3440, -0.4526, -0.8625,  1.0208, -0.6043,  0.0616,  0.2797,\n",
       "           -0.0353,  0.9191,  0.7021]]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tril(torch.ones(1,1,n_ctx, n_ctx))\n",
    "x = torch.randn((1,1,10,10))\n",
    "torch.masked_fill(x, m[:, :, :10, :10] == 0, float('-inf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
